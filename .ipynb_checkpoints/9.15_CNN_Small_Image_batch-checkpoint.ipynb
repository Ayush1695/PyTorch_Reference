{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "907eea05-9ab4-42d6-94c0-894e2dfab06e",
   "metadata": {},
   "source": [
    "<h1>Convolutional Neural Network with Batch-Normalization </h1> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199d1726-483f-463f-a3e2-599dfc8f5311",
   "metadata": {},
   "source": [
    "\n",
    "<h3>Objective for this Notebook<h3>    \n",
    "<h5> 1. Learn how to compare a Convolutional Neural Network using Batch Normalization with a regular Convolutional Neural Network  to classify handwritten digits from the MNIST database..</h5>     \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eab3361-1b3c-4350-9def-17d4bf897818",
   "metadata": {},
   "source": [
    "<h2>Table of Contents</h2>\n",
    "<b>This lab takes a long time to run so the results are given. You can run the notebook your self but it may take a long time.</b>\n",
    "<p>In this lab, we will compare a Convolutional Neural Network using Batch Normalization with a regular Convolutional Neural Network  to classify handwritten digits from the MNIST database. We will reshape the images to make them faster to process. </p>\n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"#read_me\">Read me Batch Norm for Convolution Operation  </a></li>\n",
    "<li><a href=\"#Makeup_Data\">Get Some Data</a></li>\n",
    "<li><a href=\"#CNN\">Two Types of Convolutional Neural Network</a></li>\n",
    "<li><a href=\"#Train\">Define Criterion function, Optimizer and Train the Model</a></li>\n",
    "<li><a href=\"#Result\">Analyze Results</a></li>\n",
    "</ul>\n",
    "<p>Estimated Time Needed: <strong>25 min</strong></p>\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6a44d9-3f24-451c-ae45-55ba0b3896ce",
   "metadata": {},
   "source": [
    "<h2 id=\"read_me\">Read me Batch Norm for Convolution Operation  </h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b92b19-7bda-4ba3-a521-260995c6dbe5",
   "metadata": {},
   "source": [
    "Like a fully connected network, we create a <code>BatchNorm2d</code> object, but we apply it to the 2D convolution object. First, we create objects <code>Conv2d</code> object; we require the number of output channels, specified by the variable <code>OUT</code>.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d8135f-8162-4f9e-ad91-9d89cf904cf7",
   "metadata": {},
   "source": [
    "<code>self.cnn1 = nn.Conv2d(in_channels=1, out_channels=OUT, kernel_size=5, padding=2) </code>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4a619b-8234-4313-b558-18ffac01fb74",
   "metadata": {},
   "source": [
    "We then create a Batch Norm  object for 2D convolution as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915b58c0-38e4-4712-a7a9-2a4c443c3ac5",
   "metadata": {},
   "source": [
    "<code>self.conv1_bn = nn.BatchNorm2d(OUT)</code>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399fa70a-a1b4-48a5-a3c2-104f8755a058",
   "metadata": {},
   "source": [
    "The parameter out is the number of channels in the output. We can then apply batch norm  after  the convolution operation :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783819c3-2252-4060-be15-2c5ed58effaf",
   "metadata": {},
   "source": [
    "<code>x = self.cnn1(x)</code>\n",
    "<p></p>\n",
    " <code> x=self.conv1_bn(x)</code>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bd38b3-9a9b-4a0d-a801-e7f40d574772",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcc3c7c-e6a4-4225-b65b-aa72baca8c82",
   "metadata": {},
   "source": [
    "<h2>Preparation</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7e745d-83f4-432b-a44c-cd3e59bfadb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import the libraries we need to use in this lab\n",
    "\n",
    "# Using the following line code to install the torchvision library\n",
    "# !mamba install -y torchvision\n",
    "\n",
    "!pip install torchvision==0.9.1 torch==1.8.1 \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "def show_data(data_sample):\n",
    "    plt.imshow(data_sample[0].numpy().reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')\n",
    "    plt.title('y = '+ str(data_sample[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f647dec-2b73-45a6-9c4c-7d90474b27d9",
   "metadata": {},
   "source": [
    "<h2 id=\"Makeup_Data\">Get the Data</h2> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d791b032-0137-4521-888d-3a9fd4a76de9",
   "metadata": {},
   "source": [
    "we create a transform to resize the image and convert it to a tensor :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a50307-97cd-4365-bb95-a3d5a84816bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMAGE_SIZE = 16\n",
    "\n",
    "composed = transforms.Compose([transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e913b99-b14a-493f-a1bb-7b08b6ec5a13",
   "metadata": {},
   "source": [
    "Load the training dataset by setting the parameters <code>train </code> to <code>True</code>. We use the transform defined above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09740e48-d568-4089-a81e-20f9e1cad872",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=composed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fb4e23-78e7-4d65-8af8-25904ce82268",
   "metadata": {},
   "source": [
    "Load the testing dataset by setting the parameters train  <code>False</code>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b10237-7dd3-43c4-9200-7cbe5c8d46dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the validating \n",
    "\n",
    "validation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=composed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053c5b3f-adbf-4baa-9027-a1a60b0555ee",
   "metadata": {},
   "source": [
    "We can see the data type is long.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8ed20f-deb6-4fdc-ab0d-4c1d9fe29237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the data type for each element in dataset\n",
    "\n",
    "type(train_dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2350de3-e1e9-49be-bae0-b60f2b4e63b4",
   "metadata": {},
   "source": [
    "Each element in the rectangular tensor corresponds to a number representing a pixel intensity as demonstrated by the following image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6cf390-26ca-43a2-b3c8-2e34f95a5ea9",
   "metadata": {},
   "source": [
    "<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter%206/6.2.1imagenet.png\" width=\"550\" alt=\"MNIST data image\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b519d0bf-29f0-4871-a174-0f773a05fe71",
   "metadata": {},
   "source": [
    "Print out the fourth label \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6304e8-b610-449f-9070-3934e2179b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The label for the fourth data element\n",
    "\n",
    "train_dataset[3][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d491f9-b49a-4517-8c41-32f9cc1d8b9a",
   "metadata": {},
   "source": [
    "Plot the fourth sample \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3559e1d-6e65-4d10-bb2d-f2260314cdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The image for the fourth data element\n",
    "show_data(train_dataset[3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d957d7f4-bc3a-40a1-b729-f9fd606e6fbf",
   "metadata": {},
   "source": [
    "The fourth sample is a \"1\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4f602f-496b-40e9-8d0c-6917d4d16cf8",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db198bf4-16c8-4b48-ba51-ae9087d38558",
   "metadata": {},
   "source": [
    "<h2 id=\"CNN\">Build a Two Convolutional Neural Network Class</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8f2bf5-62aa-43e4-84d5-1687b53d021d",
   "metadata": {},
   "source": [
    "Build a Convolutional Network class with two Convolutional layers and one fully connected layer. Pre-determine the size of the final output matrix. The parameters in the constructor are the number of output channels for the first and second layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33a2c35-1087-4ce4-bd49-3f949fadbb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    # Contructor\n",
    "    def __init__(self, out_1=16, out_2=32):\n",
    "        super(CNN, self).__init__()\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, padding=2)\n",
    "        self.maxpool1=nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=5, stride=1, padding=2)\n",
    "        self.maxpool2=nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(out_2 * 4 * 4, 10)\n",
    "    \n",
    "    # Prediction\n",
    "    def forward(self, x):\n",
    "        x = self.cnn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a447ece7-7ec8-4fb6-a6c5-8599a246cc93",
   "metadata": {},
   "source": [
    "Build a Convolutional Network class with two Convolutional layers and one fully connected layer. But we add Batch Norm for the convolutional layers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bcfa21-6951-4ac9-adb2-04c3d8a2a3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_batch(nn.Module):\n",
    "    \n",
    "    # Contructor\n",
    "    def __init__(self, out_1=16, out_2=32,number_of_classes=10):\n",
    "        super(CNN_batch, self).__init__()\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, padding=2)\n",
    "        self.conv1_bn = nn.BatchNorm2d(out_1)\n",
    "\n",
    "        self.maxpool1=nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv2_bn = nn.BatchNorm2d(out_2)\n",
    "\n",
    "        self.maxpool2=nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(out_2 * 4 * 4, number_of_classes)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(10)\n",
    "    \n",
    "    # Prediction\n",
    "    def forward(self, x):\n",
    "        x = self.cnn1(x)\n",
    "        x=self.conv1_bn(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.cnn2(x)\n",
    "        x=self.conv2_bn(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x=self.bn_fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846fbb8b-9f36-44d0-a3e6-4912b2f497dc",
   "metadata": {},
   "source": [
    "Function to train the model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0269a6-7b7c-444e-8b22-dfd556a16e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,train_loader,validation_loader,optimizer,n_epochs=4):\n",
    "    \n",
    "    #global variable \n",
    "    N_test=len(validation_dataset)\n",
    "    accuracy_list=[]\n",
    "    loss_list=[]\n",
    "    for epoch in range(n_epochs):\n",
    "        for x, y in train_loader:\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            z = model(x)\n",
    "            loss = criterion(z, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_list.append(loss.data)\n",
    "\n",
    "        correct=0\n",
    "        #perform a prediction on the validation  data  \n",
    "        for x_test, y_test in validation_loader:\n",
    "            model.eval()\n",
    "            z = model(x_test)\n",
    "            _, yhat = torch.max(z.data, 1)\n",
    "            correct += (yhat == y_test).sum().item()\n",
    "        accuracy = correct / N_test\n",
    "        accuracy_list.append(accuracy)\n",
    "     \n",
    "    return accuracy_list, loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098f4b3c-4c50-4fd8-a887-58c428379752",
   "metadata": {},
   "source": [
    "<h2 id=\"Train\">Define the Convolutional Neural Network Classifier, Criterion function, Optimizer and Train the Model</h2> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bc89a3-6e85-4bd6-a0fc-aba6c1dcadca",
   "metadata": {},
   "source": [
    "There are 16 output channels for the first layer, and 32 output channels for the second layer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3542a623-96b1-49ec-9adb-fd21631f46b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model object using CNN class\n",
    "model = CNN(out_1=16, out_2=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a12999-017c-4b9a-85ab-083fec760ca6",
   "metadata": {},
   "source": [
    "Define the loss function, the optimizer and the dataset loader \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5684350-9471-4b34-8121-e978d9d8fdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228b51eb-03ab-4c69-86f1-c77e119507e7",
   "metadata": {},
   "source": [
    "Train the model and determine validation accuracy technically test accuracy **(This may take a long time)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a879318-c8da-44b4-933b-23f9061f4c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "accuracy_list_normal, loss_list_normal=train_model(model=model,n_epochs=10,train_loader=train_loader,validation_loader=validation_loader,optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d3edbe-eb3c-4ed1-8561-c02befeb2816",
   "metadata": {},
   "source": [
    "Repeat the Process for the model with  batch norm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a78b59-b98c-4969-9eea-fb7f20305c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_batch=CNN_batch(out_1=16, out_2=32)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model_batch.parameters(), lr = learning_rate)\n",
    "accuracy_list_batch, loss_list_batch=train_model(model=model_batch,n_epochs=10,train_loader=train_loader,validation_loader=validation_loader,optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f77568c-da3a-4041-9f03-672b35180ca4",
   "metadata": {},
   "source": [
    "<!--Empty Space for separating topics-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1072d790-1756-45d3-b886-18fe5465ce85",
   "metadata": {},
   "source": [
    "<h2 id=\"Result\">Analyze Results</h2> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84a273d-af23-4e76-8985-f962ae66ff8a",
   "metadata": {},
   "source": [
    "Plot the loss with both networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42e4c53-db0d-45e2-9e8e-476c07efae36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss and accuracy\n",
    "\n",
    "plt.plot(loss_list_normal, 'b',label='loss normal cnn ')\n",
    "plt.plot(loss_list_batch,'r',label='loss batch cnn')\n",
    "plt.xlabel('iteration')\n",
    "plt.title(\"loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c6865b-ad31-4c74-ab69-e82d785b3a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracy_list_normal, 'b',label=' normal CNN')\n",
    "plt.plot(accuracy_list_batch,'r',label=' CNN with Batch Norm')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title(\"Accuracy \")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050bd346-422a-4769-8e34-92c354ff0ea7",
   "metadata": {},
   "source": [
    "We see the CNN with batch norm performers better, with faster convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561b5d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
